\documentclass[a4,center,fleqn]{NAR}


\usepackage{NAR-natbib}
\bibliographystyle{unsrtnat}
\usepackage{color}
\usepackage{xfrac}
\newcommand{\rngcomment}[1]{{\color{red}RNG: #1}}
\newcommand{\bkmcomment}[1]{{\color{blue}BKM: #1}}
\newcommand{\batcave}{BATCAVE }
% Enter dates of publication
\copyrightyear{2008}
\pubdate{31 July 2009}
\pubyear{2009}
\jvolume{37}
\jissue{12}

\articlesubtype{Genomics and Bioinformatics}

\begin{document}

\title{\batcave: Bayesian Analysis Tools for Context-Aware Variant Evaluation}

\author{%
Brian K. Mannakee\,$^{1}$ and
Ryan N. Gutenkunst\,$^{2}$%
\footnote{To whom correspondence should be addressed.
Email: rgutenk@email.arizona.edu}}

\address{%
$^{1}$Mel and Enid Zuckerman College of Public Health, University of Arizona, Tucson AZ
and
$^{2}$Department of Molecular and Cellular Biology, University of Arizona, Tucson AZ}
% Affiliation must include:
% Department name, institution name, full road and district address,
% state, Zip or postal code, country

\history{%
Received January 1, 2009;
Revised February 1, 2009;
Accepted March 1, 2009}

\maketitle

\begin{abstract}
Text. Text. Text. Text. Text. Text. Text. Text. Text. Text. Text.
Text. Text. Text. Text. Text. Text. Text. Text. Text. Text. Text.
Text. Text. Text. Text. Text. Text. Text. Text. Text. Text. Text.
Text. Text. Text. Text. Text. Text. Text. Text. Text. Text. Text.
Text. Text. Text. Text. Text. Text. Text. Text. Text. Text. Text.
Text. Text. Text. Text. Text. Text. Text. Text. Text. Text. Text.
Text. Text. Text. Text. Text. Text. Text. Text. Text. Text. Text.
Text. Text. Text. Text. Text. Text. Text. Text. Text. Text. Text.
Text. Text.
\end{abstract}


\section{Introduction}

Cancer develops as the result of the accumulation of somatic mutations and clonal selection of cells with mutations that confer a selective advantage on the cell.
Understanding the forces that shaped the evolutionary history of a tumor, the mutations that are responsible for its growth, the rate at which mutations are occurring, or how much genetic diversity is likely present in the tumor, requires accurate variant calling, particularly at low variant allele frequency \cite{Williams2016,Bozic2016,Williams2018,Shi2018}.
Accurate variant identification is also critical in optimizing the treatment regime for an individual patients disease \citep{Ding2012,Mardis2012,Chen2013,Borad2014,Findlay2016}.
Low frequency mutations present a significant problem for current mutation calling methods because their signature in the data is difficult to distinguish from the noise introduced by Next Generation Sequencing (NGS), and this problem increases as sequencing depth increases.

Methods for identifying true somatic mutations - i.e. variant calling -  from NGS data are an active area of research in bioinformatics.
The earliest widely used somatic variant callers aimed specifically at tumors, Mutect1 and Varscan2, used a combination of heuristic filtering and a model of sequencing errors to identify and score potential variants, setting a threshold for that score designed to balance sensitivity and specificity \citep{Koboldt2012,Cibulskis2013}.
Subsequent research gave rise to a number of alternate variant calling strategies including haplotype based callers \citep{Garrison2012},
joint genotype analysis (SomaticSniper, JointSNVMix2, Seurat, and CaVEMan,MuClone)\citep{Larson2012,Roth2012a,Christoforides2013,Jones2016,Dorri2019}, allele frequency based analysis (Strelka, MuTect, LoFreq, EBCall, deepSNV, LoLoPicker, and MuSE)\citep{Saunders2012,Wilm2012,Shiraishi2013b,Gerstung2012,Carrot-Zhang2017,Fan2016}, and a mixture of ensemble and deep learning methods (MutationSeq, SomaticSeq, SNooPer, and BAYSIC).
All of these methods have varying levels of complexity, and some are focused on specific types of data.
The one thing they all have in common is that they either implicitly or explicitly assume that the probability of a mutation occuring at a particular site is proportional to the overall mutation rate, and the same at every site in the genome.

Single nucleotide substitions, i.e. simple mutations, arise in tumors at a rate and at genomic locations driven by two main processes. 
The first is the spontaneous accumulation of mutations that occurs in all dividing tissues, and has a characteristic mutation signature that describes the probability of mutation in a given genomic context \citep{Nik-Zainal2012a,Alexandrov2015,Lee-Six2018}. 
The second -- and far more complex -- process is the accumulation of mutations through exposure to mutagens or degradation - via mutation or deletion - of cellular machinery responsible for the identification and repair of damage or replication errors. 
Many mutagens and DNA repair mechanism defects also have highly specific mutation signatures, such that they can be identified by observing the mutations in the tumor \citep{Alexandrov2013a,Helleday2014a,Nik-Zainal2016,Kandoth2013,Alexandrov2016}.
The specific processes driving mutations in a particular tumor generate a tumor's unique mutation profile.
Because profiles vary widely both within and between tumor types.

Here we present an algorithm for estimating the prior probability of mutation at a given site using the observed mutation spectrum of the tumor as well as its mutation rate, and show that the addition of this prior to the MuTect variant calling model produces a superior variant classifier in both simulated and real tumor data.
We provide a simple implementation in R that takes MuTect caller output as input, and returns the posterior probability that a site is variant for every site observed by MuTect. \bkmcomment{batcave,batcaver,batcaveR}

\section{MATERIALS AND METHODS}
\subsection{Somatic variant calling base probability model}

At every site in the genome with non-zero coverage, Next Generation Sequencing (NGS) produces a vector $\mathbf{x}  = (\{b_i\},\{q_i\}), i = 1\dots \mathrm{d}$ of base calls and their associated quality scores, where $\mathrm{d}$ is total read depth.
We want to use $\mathbf{x}$ to select between competing hypotheses;
\begin{equation}
  \label{eqn:hypothesis}
$$
  \begin{array}{l}
    \mathbf{H_0}:\quad \textrm{Alt allele} = m;\quad\nu = 0\\
    \mathbf{H_1}:\quad \textrm{Alt allele} = m;\quad\nu = \hat{f},
  \end{array}
$$
\end{equation}
where $\nu$ is the variant allele frequency, $\hat{f}$ is the maximum likelihood estimate of $\nu$ given data $\mathbf{x}$, i.e. the ratio of the count of variant reads and total read depth, and $m$ is any of the 3 possible alternate non-reference bases.
The posterior probability of a given hypothesis, $P(m,\nu)$, is the product of the likelihood of the data given the hypothesis and the prior probability of the hypothesis. 
Assuming that reads are independent, we have

\begin{equation}
  \label{eqn:1}
$$
  \mathrm{P}(m,\nu) = \mathrm{p}(m,\nu) \cdot \prod_{i=1}^{\mathrm{d}} \textrm{f}_{m,\nu}(x_i),
$$
\end{equation}
where the density $\textrm{f}_{m,\nu}(x_i)$ can take several forms.

Assuming that the identity of the alternate allele and its allele frequency are independent of each other, and $\nu$ is uniformly distributed (i.e. $p(\nu) = 1$), we can rewrite equation \ref{eqn:1} as
\begin{equation}
  \label{eqn:2}
$$
  P(m,\nu) = p(m) \cdot \prod_{i=1}^{\mathrm{d}} \textrm{f}_{m,\nu}(x_i).
$$
\end{equation}
Here we show how \batcave can be used to provide a tumor and site-specific estimate of the prior probability of mutation $p(m)$.

\subsection{Site-specific prior probability of mutation}
The probability which we have denoted $p(m)$ in equation \ref{eqn:2} for compactness is more precisely the joint probability that a mutation has occured $M$, and that it was to allele $m$, which we will denote here $p(m,M)$.
However, $p(m,M)$ is not constant for every site in the genome, and follows a distribution conditional on the genomic context surrounding the site, denoted here $p(m,M | C)$ \cite{Buisson2019}.
Assuming that $m$ and $M$ are independent conditional on the genomic context, $\mathrm{p}(m,M \mid C) = \mathrm{p}(m \mid C) \mathrm{p}(M \mid C)$, which we can use Bayes' rule to further decompose as 

\begin{equation}
  \label{eqn:full_prior}
  $$
  \mathrm{p}(m,M \mid C) = \mathrm{p}(m \mid C) \mathrm{p}(C \mid M)\frac{p(M)}{p(C)}.
  $$
\end{equation}
We next show how to estimate the quantities in equation \ref{eqn:full_prior}.

\subsection{Estimation of the mutation profile}
There are many aspects of genomic architecture which can affect the somatic mutation rate at multiple scales \cite{Buisson2019}.
Here we focus on a small-scale feature, the tri-nucleotide context, which has been shown to have a strong effect on the prior probability of mutation \citep{Nik-Zainal2012a,Alexandrov2015,Lee-Six2018}.
The tri-nucleotide context at a particular genomic site is made up of the identity of the reference base and the identity of the 3` and 5` flanking bases.
There are four possible bases {\{A,C,T,G\}} 3` and 5` of the site, leading to 16 possible flanking base combinations.
Folding the central bases to the pyrimidines leaves 2 possible reference bases at the site {\{C,T\}}, resulting in $2 \cdot 16 = 32$ possible tri-nucleotide contexts.
We define a mutation context $C$ indexed by $c=\{1 \dots 32\}$, to be the tri-nucleotide context of a particular site in the genome. 
For each context, a mutation can be to any of the three possible alternate alleles {\{A,(C/T),G\}}. 
We can thus define $\mathrm{S}$, the substitution type, indexed by $c=\{1 \dots 32\}$ contexts and $m = \{1 \dots 3\}$ alternate bases, resulting in 96 possible substitution types.
Using the substitution type $\mathrm{S}$, we can rewrite equation \ref{eqn:full_prior} as 

\begin{equation}
  \label{eqn:detailed_prior}
  $$
  \begin{array}{rcl}
  \mathrm{p}(\mathrm{S_{m,c}}) &=&  \mathrm{p}(m, M | C = c) \\
                            &=& \mathrm{p}(m \mid C = c) \mathrm{p}(C = c \mid M)\frac{p(M)}{p(C = c)},
  \end{array}
  $$
\end{equation}
where $\mathrm{p}(\mathrm{S_{m,c}})$ is the probability that a substitution of type $\mathrm{S_{m,c}}$ will occur.

\begin{figure}[t]
  \begin{center}
  \includegraphics{figures/signature_figure.pdf}
  \end{center}
  \caption{Mutation profiles. 
  \textbf{(a)} The observed mutation profile of a breast cancer sample from \cite{Shi2018}.
  \textbf{(b)} The observed mutation profile of the AML 31 primary tumor from \cite{Griffith2015}.
  \textbf{(c)} A mutation profile used for simulating tumors, made up of equal proportions of COSMIC mutation signatures 1, 7, \& 11.
  \textbf{(d)} Equal proportions of signatures 1, 4, \& 5.
  \textbf{(e)} Equal proportions of signatures 1, 3, \& 5.
  KL divergence between the estimated profile ($\boldsymbol{\pi}$) and the simulated mutation profile for \textbf{(f)} Whole exomes and \textbf{(g)} whole genomes.
  The estimated mutation profile converges quickly to the target as the number of mutations evaluated rises.
  }
  \label{NAR-sigfig}
  \end{figure}

Figure \ref{NAR-sigfig} \textbf{a-e} shows several examples of mutation profiles from both real and simulated tumors.
The mutation profiles in Figure \ref{NAR-sigfig} can be described compactly by the vector $\boldsymbol{\pi}$ where each element of $\boldsymbol{\pi}$ represents the proportion of all observed tumor mutations of substitution type $\mathrm{S}_{m,c}$.
The distribution of observed substitutions $ \mathrm{p}(\mathrm{S}) $, is multinomial with parameter $ \boldsymbol{ \pi } = \{\pi_{m,c}\} $.
In a tumor with a very large number of verified mutations $\boldsymbol{\pi}$ could be estimated directly from the observed mutation profile, but in practice we find that many elements of $\boldsymbol{\pi}$ will have $0$ or a very small number of mutations present in the tumor.
As a result we need to model $\mathrm{p}(\mathrm{S})$ as dirichlet - multinomial with pseudo-count hyper-parameter $\boldsymbol{\alpha}$, 

$$
\begin{aligned}
  \boldsymbol{\pi} \mid \boldsymbol{\alpha} &\sim \textrm{Dirichlet}(\boldsymbol{\alpha}) \\
  \mathrm{S} \mid \boldsymbol{\pi} & \sim \textrm{Multinomial}(\boldsymbol{\pi}).
\end{aligned}
$$
In \batcave we use the symmetric non-informative hyper-parameter $\boldsymbol{\alpha} = \boldsymbol{1}$, so a-priori a mutation is equally likely in any context.

In order to estimate $\boldsymbol{\pi}$, we generate a subset of variants in the tumor in which we have high confidence that the variants are present, based on an initial calculation of their likelihood give the data.
These are variants such that for any reasonable value of the site-specific prior probability of mutation, the evidence for the variant in the read data erases any doubt about the presence of the variant. 
Let $\mathrm{D}$ be the set of high confidence variant calls, and $\mathrm{s} \in \mathrm{D}$ be the substitution type of each mutation in $\mathrm{D}$.
The posterior distribution of $\boldsymbol{\pi}$ is then $\mathrm{p}(\boldsymbol{\pi} \mid \mathrm{D}) \sim \textrm{Dirichlet}(\boldsymbol{\alpha^{\prime}})$ where
  $$
    \alpha^{\prime}_{m,c} = \alpha_{m,c} + \sum\limits_{\mathrm{s} \in \mathrm{D}} \mathrm{I}\{\mathrm{s} = \mathrm{s}_{m,c}\}.
  $$
Returning to equation \ref{eqn:detailed_prior}, we can compute the posterior predictive probability $p(C = c \mid M,\mathrm{D})$ as
\begin{equation}
  \label{eqn:post_pred}
  $$
  p(C = c \mid M,\mathrm{D}) = \frac{\sum_{m}\alpha^{\prime}_{m,C = c}}{\sum_{m,c}\alpha^{\prime}_{m,c}}.
  $$
\end{equation}
The posterior probability of mutation to allele $m$ given that a mutation has occurred in context $C = c$, $p(m  \mid C = c,\mathrm{D})$ is then
\begin{equation}
  \label{eqn:to_allele}
  $$
   p(m \mid C = c,\mathrm{D}) = \frac{\alpha^{\prime}_{m,c}}{\sum_{m} \alpha^{\prime}_{m,C = c}}.
  $$
\end{equation}
The remaining context dependent component in equation \ref{eqn:detailed_prior} is $p(C = c)$, the prior probability of seeing a particular tri-nucleotide context.
We compute this as simply the proportion of all tri-nucleotide contexts with the context $C = c$ present in the target genome, which is slightly different depending on whether we are analyzing a whole genome or an exome.


\subsection{Estimation of the mutation rate.}
The final piece of equation \ref{eqn:detailed_prior} to specify is $p(M)$, the prior probability that a mutation will occur at a given site, which usually specified as the mutation \textit{rate} per base $\mu$.
In an exponentially growing and neutrally or nearly neutrally evolving tumor, \citet{Williams2018} show that the total number of mutations $\mathrm{M}$ between two allele frequencies ($f_{min}$,$f_{max}$) is
\begin{equation}
  \label{eqn:mut_rate}
$$
  \mathrm{M}(f_{min},f_{max}) = N\frac{\mu}{\beta}\left(\frac{1}{f_{min}} - \frac{1}{f_{max}}\right).
$$
\end{equation}

The number of bases $N$ is fixed at $3\cdot10^9$ for a whole genome sequence and $3\cdot10^7$ for a whole exome.
Returning to the set of high confidence mutations $\mathrm{D}$, we can set $f_{max}$ to be the largest allele frequency in $\mathrm{D}$, $f_{min}$ as the lowest, and count the number of mutations in $D$ falling between the two.
In practice it is usually best to select a conservative value for $f_{min}$, which is strongly dependent on sequencing depth, and we include this as a parameter in the R implementation of \batcave.
In all results presented in this paper we have set $f_{min}$ to 0.05 because we are working with very high sequencing depth.
A value 0.12 for $f_{min}$ at lower sequencing depth is suggested in \citet{Williams2016}.
The quantity $\mu/\beta$ is the effective mutation rate, where $\beta$ is the fraction of cell divisions that lead to two surviving lineages.
We make the simplifying assumption that there is no cell death, so that $\beta = 1$, which for most plausible values of $\beta$ means we over-estimate $\mu$ by a small amount.
Equation \ref{eqn:mut_rate} then allows us to estimate $p(M) = \mu$, and we have a complete tumor-specific specification of the site-specific prior probability of mutation.

\subsection{The likelihood function}
We designed \batcave to be used as a post-calling evaluation tool with MuTect
MuTect is particularly useful because it directly reports the log ratio of the of the likelihood functions for the null and alterative hypotheses in \ref{eqn:hypothesis} as either \texttt{TLOD} (MuTect1) or \texttt{t\_lod\_fstar} (MuTect2).
We used MuTect 1.1.7 for all of the analysis in this paper, so the output of MuTect provides us with

\begin{equation}
  \label{eqn:tlod}
    $$
    \mathrm{TLOD} = \frac{\prod_{i=1}^{\mathrm{d}} \textrm{f}_{m,\nu = \hat{f}}(x_i)}{\prod_{i=1}^{\mathrm{d}} \textrm{f}_{m,\nu = 0}(x_i)}.
    $$
\end{equation}

The log posterior odds is the log likelihood ratio (\texttt{TLOD}) + the log prior odds, so we can compute the posterior odds in favor of the alternate hypothesis for a given substitution type as

\begin{equation}
  \label{eqn:computed_posterior}
  $$
  \frac{\mathrm{P}(m,\nu = \hat{f})}{1 - \mathrm{P}(m,\nu = \hat{f})} = 10^{\mathrm{TLOD} + \mathrm{logit}_{10}(\mathrm{p}(\mathrm{S_{m,c}}))},
  $$
\end{equation}
where $\mathrm{p}(\mathrm{S_{m,c}})$ is the prior probability of a substitution of type $\mathrm{S_{m,c}}$, as described in equation \ref{eqn:detailed_prior} and specified in equations \ref{eqn:post_pred} - \ref{eqn:mut_rate}.
When comparing our posterior odds to those of MuTect in this paper, we use the assumption put forward by the MuTect authors in \citet{Cibulskis2013}.
They suggest a threshold for the posterior odds of 2, and a prior per-base probability of mutation of $3\cdot10^{-6}$, leading to a \texttt{TLOD} threshold of 6.3.
We compute the posterior odds for Mutect as
\begin{equation}
  \label{eqn:mutect_posterior}
  $$
  \frac{\mathrm{P}(m,\nu = \hat{f})}{1 - \mathrm{P}(m,\nu = \hat{f})} = 10^{\mathrm{TLOD} - 6},
  $$
\end{equation}
since $\mathrm{logit}_{10}(3\cdot10^{-6}) \approx -6$, and $10^{.3} \approx 2$.

\subsection{Tumor simulations.}
We simulated realistic variant sites and allele frequencies using a branching process to simulate neutral evolution with no death.
Tumors were simulated with three different mutation profiles, all of which include a contribution from COSMIC mutation signature 1 which is found in nearly all tumors and is associated with spontaneous cytosine deamination.
The "Concentrated" profile is simulated with $\mathbf{\pi}$ set to an equal combination of the COSMIC signatures 1, 7, and 11, which has a large percentage of C $>$ T substitutions such as are often seen in cancers caused by UV exposure.
The "Intermediate" profile is simulated with $\mathbf{\pi}$ set to an equal combination of the COSMIC signatures 1, 4, and 5, which has been associated with tobacco carcinogens and is representative of some lung cancers.
The "Diffuse" profile is simulated with $\mathbf{\pi}$ set to an equal combination of the COSMIC signatures 1, 3, and 5, which has been associated with inactivating germline mutations in the BRCA1/2 genes leading to a deficiency in DNA double strand break repair.
The simulated mutation rate was $\mu = 3\cdot10^{-6}$.
Variants were selected from a combination of Cancer Genome Atlas (TCGA) and Pan-cancer Analysis of Whole Genomes (PCAWG) databases, which include mutations found in all types of cancer.
Whole genome (100X depth), and whole exome (500X depth) reads were simulated from the GRCH38 reference genome using VarSim \cite{Mu2015}, and aligned with BWA \cite{Li2009a}, both with default parameters.
Variants were spiked to create tumors with Bamsurgeon with default parameters \cite{Ewing2015a},
and called with MuTect 1.1.7 \cite{Cibulskis2013} with the following parameters:

\begin{tiny}
\begin{verbatim}

  java -Xmx24g -jar $MUTECT_JAR --analysis_type MuTect --reference_sequence $ref_path \
        --dbsnp $db_snp \
        --enable_extended_output \
        --fraction_contamination 0.00 \
        --tumor_f_pretest 0.00 \
        --initial_tumor_lod -10.00 \
        --required_maximum_alt_allele_mapping_quality_score 1 \
        --input_file:normal $tmp_normal \
        --input_file:tumor $tmp_tumor \
        --out $out_path/$chr.txt \
        --coverage_file $out_path/$chr.cov

\end{verbatim}
\end{tiny}
Variants identified by MuTect are labelled as to whether they pass all MuTect filters, pass all filters *other* than the evidence threshold \textrm{tlod\_f\_star}, or fail to pass any filter other than \textrm{tlod\_f\_star}. Variants that pass all filters or fail only \textrm{tlod\_f\_star} are then passed to {method} for prior estimation and rescoring.

\subsection{Real Data}
\begin{itemize}
  \item aml31 \cite{Griffith2015}
  \item item used only the platinum calls for evaluation
  \item Shi2018 \cite{Shi2018}
  \item used the gold standard calls, and combined all six biological replicates 
\end{itemize}

\subsection{Calibration metric}

	We use the Integrated Calibration Index to quantify the difference between MuTect and the Calibrated model \cite{Austin2019}.
	Briefly, a loess-smoothed regression is fit by regressing the binary (T/F) variant classification against the reported posterior probability of a true variant for both MuTect and the Calibrated model (Figure \ref{NAR-wes_fig} \textbf{e}).
	Plotting the smoothed regression predictions against the actual predictions would result in a diagonal line, and the Integrated Calibration Index is the mean area between the result and a diagonal line.

\section{RESULTS}
We designed \batcave as a post-call variant evaluation algorithm to be used with MuTect (Versions 1.1.7 or $>$2.0) \cite{Cibulskis2013}.
MuTect is widely used, has state of the art sensitivity and specificity, and includes numerous heuristic filters and alignment adjustments that reduce the prevalence of sequencing errors in results.
\batcave takes the output from MuTect, extracts the log-likelihood ratio for each potential variant site, and uses that ratio to separate the potential sites in to high-confidence and low-confidence groups.
The mutation profile is computed from the high confidence sites, and the posterior probability of mutation is recomputed for all sites as described in Materials and Methods.
\begin{figure*}
  \begin{center}
  \includegraphics{figures/fig_wes.pdf}
  \end{center}
  \caption{Model performance on 500X simulated whole exome.
  \textbf{(a)} Precision recall curves and \textbf{(c)} Receiver operating characteristic curves for 3 mutation signatures.
  Distribution of estimated true positive probabilities for true positive (top) and true negative (bottom) variants for \textbf{b)} the MuTect model and \textbf{d)} the \batcave model.
  A perfectly calibrated model would generate the diagonal line.}
\label{NAR-wes_fig}
\end{figure*}
\subsection{Convergence of the site-specific prior to the data generating distribution}

The context-dependent prior probability of mutation, the heart of \batcave, depends on the prior converging to a good representation of the data generating distribution within the set of high-confidence mutations.
We use the Kullback-Leibler divergence as a measure of the distance (or entropy difference) between the estimated prior and the actual simulated data generating distribution.
In Figure \ref{NAR-sigfig} \textbf{e} and \textbf{f} we order mutations by descending MuTect probability value, and compute the KL divergence between the prior after adding each mutation and the data generating distribution.
For both whole exome and whole genome the prior converges to the data generating distribution easily within the number of available high-confidence mutations. 

\subsection{Simulated data}
We generated six different tumor/normal pairs of two types, 3 each of 100X depth whole genomes and 500X whole exomes.
For each of the two types we simulated tumors with variants drawn from three different mutation profiles chosen to resemble melanoma (concentrated), a lung cancer (intermediate), and a BRCA-driven breast cancer (diffuse)  as described in Materials \& Methods and Figure \ref{NAR-sigfig}.
These 6 simulated tumor/normal pairs were used to assess the effect of sequencing depth and mutation profile type on the results obtained from \batcave.


We assessed classification performance with both the area under the ROC curve and the area under the precision-recall curve, because the classes here are deeply unbalanced.
By both metrics \batcave outperforms MuTect (Figure \ref{NAR-wes_fig} \textbf{a} and \textbf{c}, and Table \ref{table:01}).
The extent to which \batcave outperforms is dependent on both the sequencing depth and the level of concentration of the mutation signature.
Deeper sequencing and more concentrated mutation signatures increase the performance advantage of \batcave. 

The estimated mutation rate is approximately $3\cdot10^{-7}$ for all simulated tumors (Table \ref{table:01}).
We note that the effective mutation rate is lower than the simulated rate of $3\cdot10^{-6}$.
In this case the lower effective rate is not due to normal contamination of the tumor sample, but the fact that \texttt{Bamsurgeon} has restrictions -- such as sequencing depth and quality -- that prevent 100\% of simulated variants from being inserted. 

We measured calibration performance using the Integrated Calibration Index (ICI) described in Methods \citep{Austin2019}.
The ICI is a measure of the difference between predicted and observed probabilities that is weighted by the empirical density of the predicted probabilities.
This is useful in our setting because the density of predicted probabilities tends to be strongly bi-modal.
As we can see in Figure \ref{NAR-wes_fig} \textbf{b} and \textbf{d}, \batcave tends to increase posterior probabilities of low probability but true variants (top density curves), and decrease the probability of low probability but false variants (bottom density curves.).
For 500X tumors the calibration of \batcave is better across the full spectrum of posterior probabilities, while for 100X whole genomes the calibration is slightly worse.
This effect is likely due to the fact that there are simply very few low probability true positive variants in tumor sequenced to 100X depth.
As with all other metrics, the benefit of \batcave increases as the concentration of the mutation profile increases, and as sequencing depth increases.


\subsection{Real tumor data.}
We utilized two real data sets.
In each, deep sequencing and variant validation were performed with the specific purpose of evaluating tumor variant calling pipelines.
These are among a very small number of tumors which have been sequenced at high depth, and for which variant validation was undertaken.
\citet{Griffith2015} sequenced the whole genome of an acute myeloid leukemia (AML) primary tumor to a depth of $>360\mathrm{X}$, and used targeted sequencing to validate nearly 200,000 mutations.
We downloaded aligned reads for this tumor, and called variants with MuTect 1.1.7 as described in Materials \& Methods.
We estimated a per-base mutation rate for this tumor of $3\cdot10^{-8}$, which is line with the mutation rates generally considered normal in AML.
The area under the precision-recall curve is essentially identical (.995 \& .996) for MuTect and \batcave (RD figure coming and Table \ref{table:01}).

\citet{Shi2018} performed multi-region whole exome sequencing on 6 individual breast tumors to a mean target sequencing depth of $160\mathrm{X}$, and validated all variants identified by three different variant calling pipelines.
We downloaded aligned reads for this tumor, and called variants with MuTect 1.1.7 as described in Materials \& Methods.
We estimated a per-base mutation rate for this tumor of $3\cdot10^{-8}$, and the area under the precision-recall curve is identical (.972) for MuTect and \batcave.



\begin{table*}[b]
\tableparts{%
\caption{This is a table with $\mu$, auroc, auprc, ICI, and maybe fraction called?}
\label{table:01}%
}{%
\begin{tabular*}{\textwidth}{@{}lllllllll@{}}
\toprule
Depth & Mutation profile & $\mu$ & AUROC & & AUPRC & & ICI & 
\\
& & (estimated) & MuTect & \batcave & MuTect & \batcave & MuTect & \batcave
\\
\colrule
100X whole genome & Concentrated & 3.6e-7 & .987 & .993 & .972 & .975 & -- & --
\\
100X whole genome & Intermediate & 3.2e-7 & .987 & .989 & .972 & .973 & -- & --
\\
100X whole genome & Diffuse & 3.2e-7 & .988 & .989 & .971 & .973 & -- & --
\\
500X whole exome & Concentrated & 3.6e-7 & .848 & .929 & .674 & .758 & .0585 & .0462
\\
500X whole exome & Intermediate & 3.6e-7 & .847 & .881 & .677 & .706 & .0587 & .0568 
\\
500X whole exome & Diffuse & 3.6e-7 & .850 & .873 & .676 & .698 & .0565 & .0576
\\
\citet{Williams2016} & Actual & 3.6e-8 & -- & -- & .995 & .996 & -- & --
\\
\citet{Shi2018} & Actual & 3.6e-8 & -- & -- & .972 & .972 & -- & --
\\
\botrule
\end{tabular*}%
}
{$\mu$ = per-base mutation rate, AUROC/AUPRC = area under ROC/PRC curve, ICI = Integrated calibration index.}
\end{table*}


\section{DISCUSSION}

\batcave is a post-call variant evalutation tool that 
We demonstrated here that this method improves classification accuracy in both simulated data, with the magnitude of the improvement dependent on the sequencing depth and the concentration of the mutation profile.

We report only precision-recall comparisons for real tumor data because in both cases the "gold" standard data sets consist of variants that were "called" by one or more variant detection pipelines, and confirmed to be present or absent.
These mutations are in some sense \textit{easy} to call, since at least one variant caller did in fact call them.
At the same time, there is no way to compute true and false negative fractions for these data sets.
As such, it is not surprising that the performance of MuTect and \batcave are similar because the universe of variants under consideration in these experiments are by definition "callable" by at least one variant detection pipeline, and MuTect is among the most sensitive and specific variant callers \cite{Griffith2015,Cibulskis2013}.
Deep sequencing experiments where a large random sample of uncalled potential variants, as well as a random sample of variant with no reads at all present, would give much-needed insight into the differences in statistical models in variant calling.


\begin{itemize}
  \item Every tumor is different. Differences within and between tumor types are pronounced \cite{Stephens2005, Burrell2013a, Nakamura2015, Witkiewicz2015, Kumar2016}
  \item The solution must be specific to each tumor we examine
  \item The fact that we can think of tumor variant calls as two sets of data, with different measurement error, derived from the same data generating process, makes this possible
  \item The mutation profile is estimated from the high-confidence (low measurement error) variants, and used as a prior for the low-confidence (high measurement error) variants
  \item We demonstrate that the mutation generating process (mutation profile) can be reliably retrieved from simulated NGS data.
  \item We also demonstrate that on the limited real data available, which consists of validated high confidence mutations, the results are equivalent
  \item What is needed is validation experiments that also allow for the estimation of true and false negative rates, in order to fully illuminate the utility of \batcave
  \item We have devoloped this algorithm as a post-calling evaluation tool to be used at the end of a variant calling pipeline
  \item The algorithm has low computational overhead, and in principle there should be no bar to simply incorporating it into existing variant calling algorithms.
  \item The benefit from the algorithm varies with both the concentration of the signature and the depth of sequencing
  \item Deeper sequencing leads to more benefit from the algorithm because there are simply more low frequency, and thus more ambiguous, variants accessible in deeply sequenced tumors
  \item Deep sequencing is useful \cite{Zehir2017}
  \item We hope that the availability of this algorithm will have a positive effect on the cost/benefit ratio of deeper sequencing in both research and clinical applications
  \item One size does not fit all with respect to the evidence threshold. Users are interested in true and false positive fractions, and users of different types will have different trade-offs in terms of sensitivity and specificty
  \item Clinical studies often threshold at a vaf, rather than using the evidence threshold of the caller. 272X 5\% cutoff \cite{Parsons2016}, why go so deep?
  \item One problem with this is that vaf and associated evidence is dependent on local depth. This is no different than just having a minimum number of variant reads. Why have a model?
  \item We believe the improved calibration achieved with this algorithm will encourage more thoughtful consideration of thresholds in various variant calling contexts
  \item The field believes that little information can be gained by wes rather than panels, but they are assuming the only information is mutations \cite{Hyman2015}
\end{itemize}


\section{CONCLUSION}

Text. Text. Text. Text. Text. Text. Text. Text. Text. Text. Text.
Text. Text. Text. Text. Text. Text. Text. Text. Text. Text. Text.
Text. Text. Text. Text. Text. Text. Text. Text. Text. Text. Text.
Text. Text. Text. Text. Text. Text. Text. Text. Text. Text. Text.
Text. Text. Text. Text. Text. Text. Text. Text. Text. Text. Text.
Text. Text. Text. Text. Text. Text. Text. Text. Text. Text. Text.
Text. Text. Text. Text. Text. Text. Text. Text. Text. Text. Text.
Text. Text. Text. Text. Text. Text. Text. Text. Text. Text. Text.
Text. Text. Text. Text. Text. Text. Text. Text. Text. Text. Text.
Text. Text. Text.


\section{ACKNOWLEDGEMENTS}

Text. Text. Text. Text. Text. Text. Text. Text. Text. Text. Text.
Text. Text. Text. Text.


\subsubsection{Conflict of interest statement.} None declared.




\bibliography{caller_paper}

\section{Supplementary Figures}

\begin{figure*}[t]
  \begin{center}
  \includegraphics{figures/aml_plot.pdf}
  \end{center}
  \caption{Model performance on 300X Whole leukemia genome.
  \textbf{(a)} Precision/recall curves and \textbf{(c)} Reciever operating characteristic curves.
  Distribution of estimated true positive probabilities for true positive (top) and true negative (bottom) variants for \textbf{b)} the MuTect model and \textbf{d)} the Calibrated model.
  A perfectly calibrated model would generate the diagonal line.}
  \label{NAR-aml}
\end{figure*}

\begin{figure*}
  \begin{center}
  \includegraphics{figures/fig_wes.pdf}
  \end{center}
  \caption{Model performance on 100X simulated whole genome.
  \textbf{(a)} Precision recall curves and \textbf{(c)} Reciever operating characteristic curves for 3 mutation signatures.
  Distribution of estimated true positive probabilities for true positive (top) and true negative (bottom) variants for \textbf{b)} the MuTect model and \textbf{d)} the \batcave model.
  A perfectly calibrated model would generate the diagonal line.\bkmcomment{change the axis on b and d to be the same as the steyerberg paper. much clearer}}
\label{NAR-wgs_fig}
\end{figure*}
\end{document}
